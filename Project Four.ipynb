{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://www.indeed.com/jobs?q=data+scientist+$20,000&l=New+York&start=30\n",
    "# $20000 means 20k or up\n",
    "# l = location\n",
    "# start increases incrementally by 10. Starting at 00."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trialing one page of Indeed to test if my functions work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test one page of url\n",
    "url = 'https://www.indeed.com/jobs?q=data+scientist&start=00'\n",
    "html = urllib.urlopen(url).read()\n",
    "soup = BeautifulSoup(html, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "break\n"
     ]
    }
   ],
   "source": [
    "rows = soup.find_all('span', {'class':'np'})\n",
    "if len(rows) > 1:\n",
    "    print 'pass'\n",
    "elif 'Next' in rows[0].text:\n",
    "    print 'break'\n",
    "else:\n",
    "    print 'what'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Next\\xa0\\xbb'"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function making\n",
    "Defining all my functions for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Scientist\n",
      "Data Scientist (work from home)\n",
      "Data Scientist - Videa, LLC\n",
      "Data Scientist\n",
      "Data Scientist\n",
      "Data Scientist\n",
      "Data Scientist, Analytics\n",
      "Data Scientist\n",
      "Data Scientist, Risk Analytics\n",
      "Data Scientist\n",
      "Data Scientist/Quantitative Analyst, Engineering\n",
      "Data Scientist and Machine Learning Researcher\n",
      "Data Scientist\n",
      "Data Scientist\n",
      "Data Scientist\n"
     ]
    }
   ],
   "source": [
    "# Trying to get all jobs titles first\n",
    "job_list = []\n",
    "for row in soup.find_all('div', {'id':True, 'data-jk':True}):\n",
    "    jobtitle = row.a.text\n",
    "    print jobtitle\n",
    "    job_list.append(jobtitle)\n",
    "    \n",
    "# Convert to function\n",
    "def job_func(soup, job_list=[]):\n",
    "    for row in soup.find_all('div', {'id':True, 'data-jk':True}):\n",
    "        jobtitle = row.a.text\n",
    "        job_list.append(jobtitle)\n",
    "    return job_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arlington, VA 22209\n",
      "San Jose, CA\n",
      "Atlanta, GA\n",
      "Atlanta, GA\n",
      "Kennesaw, GA\n",
      "Kansas City, MO\n",
      "New York, NY\n",
      "Salt Lake City, UT\n",
      "Johnston, IA 50131\n",
      "Topeka, KS 66603\n",
      "Seattle, WA\n",
      "New York, NY\n",
      "New York, NY 10014 (West Village area)\n",
      "Reno, NV\n",
      "Durham, NC\n"
     ]
    }
   ],
   "source": [
    "# Looking at location\n",
    "location_list = []\n",
    "for row in soup.find_all('div', {'id':True}):\n",
    "    for header in row.find_all('span', {'class':'location'}):\n",
    "        location = header.text\n",
    "        print location\n",
    "        location_list.append(location)\n",
    "        \n",
    "# Convert to function\n",
    "def loc_func(soup, location_list=[]):\n",
    "    for row in soup.find_all('div', {'id':True}):\n",
    "        for header in row.find_all('span', {'class':'location'}):\n",
    "            location = header.text\n",
    "            location_list.append(location)\n",
    "    return location_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no salary info\n",
      "no salary info\n",
      "no salary info\n",
      "no salary info\n",
      "no salary info\n",
      "no salary info\n",
      "no salary info\n",
      "$75,900 - $148,100 a year\n",
      "no salary info\n",
      "$48,300 - $60,400 a year\n",
      "no salary info\n",
      "no salary info\n",
      "no salary info\n",
      "no salary info\n",
      "no salary info\n"
     ]
    }
   ],
   "source": [
    "# Looking at salary if there is one\n",
    "salary_list = []\n",
    "for row in soup.find_all('div', {'id':True}):\n",
    "    for block in row.find_all('td', {'class':'snip'}):\n",
    "        try:\n",
    "            salary =  block.find('span', {'class':'no-wrap'}).text\n",
    "            print salary\n",
    "            salary_list.append(salary)\n",
    "        except:\n",
    "            print 'no salary info'\n",
    "            salary_list.append(np.nan)\n",
    "            \n",
    "def sal_func(soup, salary_list=[]):\n",
    "    for row in soup.find_all('div', {'id':True}):\n",
    "        for block in row.find_all('td', {'class':'snip'}):\n",
    "            try:\n",
    "                salary =  block.find('span', {'class':'no-wrap'}).text\n",
    "                salary_list.append(salary)\n",
    "            except:\n",
    "                salary_list.append(np.nan)\n",
    "                \n",
    "    return salary_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Looking at company\n",
    "company_list = []\n",
    "for row in soup.find_all('div', {'id':True}):\n",
    "    for header in row.find_all('span', {'class':'company'}):\n",
    "        company_list.append(header.text.replace('\\n',''))\n",
    "\n",
    "def comp_func(soup, company_list=[]):\n",
    "    for row in soup.find_all('div', {'id':True}):\n",
    "        for header in row.find_all('span', {'class':'company'}):\n",
    "            company_list.append(header.text.replace('\\n',''))\n",
    "    return company_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sponsored\n",
      "Sponsored\n",
      "Sponsored\n",
      "2 days ago\n",
      "1 day ago\n",
      "3 days ago\n",
      "5 hours ago\n",
      "17 hours ago\n",
      "2 hours ago\n",
      "23 hours ago\n",
      "13 hours ago\n",
      "2 days ago\n",
      "2 days ago\n",
      "8 hours ago\n",
      "Sponsored\n"
     ]
    }
   ],
   "source": [
    "# Looking at days posted/sponsored. Will specify sponsored as np.nan\n",
    "# Time is organized by: hours, days, and more than 30 days. 30+ days will be regarded as 30 days\n",
    "time_list = []\n",
    "for row in soup.find_all('div', {'id':True}):\n",
    "    for header in row.find_all('div', {'class':'result-link-bar'}):\n",
    "        try:\n",
    "            time =  header.find('span', {'class':'date'}).text\n",
    "            print time\n",
    "            if 'days' in time:\n",
    "                days = float(time.split()[0])\n",
    "                time_list.append(round(days,2))\n",
    "            elif 'hours' in time:\n",
    "                hours = float(time.split()[0])/24.\n",
    "                time_list.append(round(hours,2))\n",
    "            else:\n",
    "                time_list.append(30.0)\n",
    "        except:\n",
    "            print 'Sponsored'\n",
    "            time_list.append(np.nan)\n",
    "            \n",
    "def time_func(soup, time_list=[]):\n",
    "    for row in soup.find_all('div', {'id':True}):\n",
    "        for header in row.find_all('div', {'class':'result-link-bar'}):\n",
    "            try:\n",
    "                time =  header.find('span', {'class':'date'}).text\n",
    "                if 'days' in time:\n",
    "                    days = float(time.split()[0])\n",
    "                    time_list.append(days)\n",
    "                elif 'hours' in time:\n",
    "                    hours = float(time.split()[0])/24.\n",
    "                    time_list.append(hours)\n",
    "                else:\n",
    "                    time_list.append(30.0)\n",
    "            except:\n",
    "                time_list.append(np.nan)\n",
    "                \n",
    "    return time_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make dataframe of information\n",
    "# Lists are:\n",
    "'''job_list, location_list, salary_list, company_list, time_list, rating_list, summary_list\n",
    "rating_list is a tuple with two pieces of info: rating out of 5 and number of ratings'''\n",
    "\n",
    "# Split rating_list into 2 parts: star_list and review_list\n",
    "star_list = [i[0] for i in rating_list]\n",
    "review_list = [i[1] for i in rating_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get summary of job posting\n",
    "summary_list = []\n",
    "for row in soup.find_all('div', {'id':True}):\n",
    "    for header in row.find_all('span', {'class':'summary'}):\n",
    "        summary_list.append(header.text.replace('\\n',''))\n",
    "\n",
    "def sum_func(soup, summary_list=[]):\n",
    "    for row in soup.find_all('div', {'id':True}):\n",
    "        for header in row.find_all('span', {'class':'summary'}):\n",
    "            summary_list.append(header.text.replace('\\n',''))\n",
    "    return summary_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ratings and number of reviews\n",
    "rating_list = []\n",
    "for row in soup.find_all('div', {'id':True, 'data-jk':True}):\n",
    "    try:\n",
    "        # Get number of ratings\n",
    "        ratings = row.find('span', {'class':'slNoUnderline'}).text\n",
    "        \n",
    "        # Get actual star count out of 5\n",
    "        # width is a string with format 'width:##.#px'. Trying to pull out ##.# as a float\n",
    "        width = row.find('span', {'style':True})['style']\n",
    "        pixels = re.search('\\d{2}.\\d',width)\n",
    "        stars = round(float(pixels.group())/60. * 5, 2)\n",
    "        rating_list.append((stars, ratings))\n",
    "        \n",
    "    except:\n",
    "        rating_list.append((np.nan, np.nan))\n",
    "\n",
    "def rat_func(soup, rating_list=[], star_list=[], review_list=[]):\n",
    "    for row in soup.find_all('div', {'id':True, 'data-jk':True}):\n",
    "        try:\n",
    "            # Get number of ratings\n",
    "            ratings = row.find('span', {'class':'slNoUnderline'}).text\n",
    "\n",
    "            # Get actual star count out of 5\n",
    "            # width is a string with format 'width:##.#px'. Trying to pull out ##.# as a float\n",
    "            width = row.find('span', {'style':True})['style']\n",
    "            pixels = re.search('\\d{2}.\\d',width)\n",
    "            stars = round(float(pixels.group())/60. * 5, 2)\n",
    "            rating_list.append((stars, ratings))\n",
    "\n",
    "        except:\n",
    "            rating_list.append((np.nan, np.nan))\n",
    "            \n",
    "    star_list = [i[0] for i in rating_list]\n",
    "    review_list = [i[1] for i in rating_list]\n",
    "            \n",
    "    return star_list, review_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(zip(job_func(soup), \n",
    "#                       loc_func(soup), \n",
    "#                       sal_func(soup), \n",
    "#                       comp_func(soup), \n",
    "#                       time_func(soup), \n",
    "#                       rat_func(soup)[0], \n",
    "#                       rat_func(soup)[1],\n",
    "#                       sum_func(soup)), \n",
    "#                   columns=['Title', \n",
    "#                            'Location', \n",
    "#                            'Salary',\n",
    "#                            'Company', \n",
    "#                            'Post_date', \n",
    "#                            'Rating', \n",
    "#                            'Reviews',\n",
    "#                            'Summary'])\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Got some information, but df is not clean at all. Let's work easiest to hardest. \n",
    "# Reviews is first\n",
    "\n",
    "def cleaner(row):\n",
    "    try:\n",
    "        # Replace commas with nothing\n",
    "        new_row = row.replace(',', '')\n",
    "\n",
    "        # Get of rid of word 'reviews'\n",
    "        new_row = float(new_row.split()[0])\n",
    "    except:\n",
    "        new_row = np.nan    \n",
    "    \n",
    "    return new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a cleaner for salary. Taking average of range of salary for computation purposes\n",
    "def sal_cleaner(row):\n",
    "    try:\n",
    "        # Replace commas with nothing\n",
    "        new_row = row.replace(',', '')\n",
    "\n",
    "        # Get list of salary split by -\n",
    "        split_list = new_row.split('a year')[0]\n",
    "        dol_replace = split_list.replace('$', '')\n",
    "        dol_split = dol_replace.split('-')\n",
    "        \n",
    "        if len(dol_split) > 1:\n",
    "            new_row = 0.5 * (float(dol_split[0]) + float(dol_split[1]))\n",
    "        else:\n",
    "            new_row = float(dol_split[0])\n",
    "    except:\n",
    "        new_row = np.nan    \n",
    "    \n",
    "    return new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We are going to eventually run this above code multiple times. So better to put in a function\n",
    "def indeed_maker(soup):\n",
    "    df = pd.DataFrame(zip(job_func(soup), \n",
    "                      loc_func(soup), \n",
    "                      sal_func(soup), \n",
    "                      comp_func(soup), \n",
    "                      time_func(soup), \n",
    "                      rat_func(soup)[0], \n",
    "                      rat_func(soup)[1],\n",
    "                      sum_func(soup)), \n",
    "                  columns=['Title', \n",
    "                           'Location', \n",
    "                           'Salary',\n",
    "                           'Company', \n",
    "                           'Post_date', \n",
    "                           'Rating', \n",
    "                           'Reviews',\n",
    "                           'Summary'])\n",
    "    \n",
    "    df.Reviews = df.Reviews.map(cleaner)\n",
    "    df.Salary = df.Salary.map(sal_cleaner)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to run. Tried to run a while loop, but Indeed cuts off at page 100.\n",
    "Even if there are more than 100 page worths, Indeed cuts off.\n",
    "As a result, will have to search through several cities instead of searching without locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_of_cities = ['Atlanta', 'Los+Angeles', 'Seattle', 'Portland', 'San+Francisco', \n",
    "                  'Washington%2C+DC', 'Boston', 'Austin', 'Dallas', 'Houston',\n",
    "                  'Orlando', 'Philadelphia', 'Pittsburgh', 'Cincinnati', 'Miami',\n",
    "                  'New+York+City', 'New+Jersey', 'Kansas+City', 'San Diego',\n",
    "                 'San+Jose', 'Salt+Lake+City', 'Raleigh', 'Minneapolis', 'Oklahoma+City',\n",
    "                 'St.+Louis', 'Detroit', 'Colombus', 'Cleveland', 'Indianapolis', 'Baltimore']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-380-bd0da06fa5a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m             \u001b[1;32melif\u001b[0m \u001b[1;34m'Previous'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrows\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Looking for two titles\n",
    "for role in ['data+scientist', 'data+analyst']:\n",
    "    # Looking at a list of cities\n",
    "    for city in list_of_cities:\n",
    "        # Looking through all pages Indeed will allow you to look through\n",
    "        for page in np.linspace(0, 950, 20):\n",
    "            # Page indicator\n",
    "            start = str(int(page))\n",
    "\n",
    "            # Web initializer\n",
    "            url = 'https://www.indeed.com/jobs?q=' + role + '&l=' + city + '&limit=50&start=' + start\n",
    "            html = urllib.urlopen(url).read()\n",
    "            soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "            # Function that scrapes all relevant data and creates a dataframe        \n",
    "            ind_df = indeed_maker(soup)\n",
    "            \n",
    "            # Break for loop if no next link is available\n",
    "#             rows = soup.find_all('span', {'class':'np'})\n",
    "#             if len(rows) > 1:\n",
    "#                 pass\n",
    "#             elif 'Previous' in rows[0].text:\n",
    "#                 break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting rid of row duplicates (sometimes reviews and rating will be different)\n",
    "ind_df.drop_duplicates(subset=['Title', 'Location', 'Salary', 'Company', 'Summary'], \n",
    "                       inplace=True)\n",
    "ind_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Saving dataframe as a csv so that I don't have to run code again.\n",
    "ind_df.to_csv('Indeed_0616', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = 'Indeed_0616'\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
