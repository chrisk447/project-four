{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://www.indeed.com/jobs?q=data+scientist+$20,000&l=New+York&start=30\n",
    "# $20000 means 20k or up\n",
    "# l = location\n",
    "# start increases incrementally by 10. Starting at 00."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test one page of url\n",
    "url = 'https://www.indeed.com/jobs?q=data+scientist+$20,000&start=00'\n",
    "html = urllib.urlopen(url).read()\n",
    "soup = BeautifulSoup(html, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Scientist\n",
      "Data Scientist\n",
      "Data Scientist\n",
      "Data Scientist, North America Supply Chain Advanced Analytic...\n",
      "Data Scientist\n",
      "Data Scientist - Healthcare Remote USA\n",
      "Data Scientist/Machine Learning Engineer\n",
      "Data Scientist, Analytics, University Graduate\n",
      "Data Scientist\n",
      "Entry Level Data Analysis\n",
      "Freelance Data Scientist\n",
      "Junior Data Scientist\n",
      "Data Scientist, Advanced Analytics\n",
      "Data Scientist\n",
      "Data Scientist\n"
     ]
    }
   ],
   "source": [
    "# Trying to get all jobs titles first\n",
    "job_list = []\n",
    "for row in soup.find_all('div', {'id':True, 'data-jk':True}):\n",
    "    jobtitle = row.a.text\n",
    "    print jobtitle\n",
    "    job_list.append(jobtitle)\n",
    "    \n",
    "# Convert to function\n",
    "def job_func(soup, job_list=[]):\n",
    "    for row in soup.find_all('div', {'id':True, 'data-jk':True}):\n",
    "        jobtitle = row.a.text\n",
    "        job_list.append(jobtitle)\n",
    "    return job_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arlington, VA 22209\n",
      "Durham, NC\n",
      "Harrisburg, PA\n",
      "Portland, OR\n",
      "United States\n",
      "Ann Arbor, MI\n",
      "San Diego, CA 92129\n",
      "Menlo Park, CA\n",
      "San Diego, CA\n",
      "Germantown, MD\n",
      "Remote\n",
      "San Francisco, CA\n",
      "Billerica, MA\n",
      "Lehi, UT\n",
      "Austin, TX 78731\n"
     ]
    }
   ],
   "source": [
    "# Looking at location\n",
    "location_list = []\n",
    "for row in soup.find_all('div', {'id':True}):\n",
    "    for header in row.find_all('span', {'class':'location'}):\n",
    "        location = header.text\n",
    "        print location\n",
    "        location_list.append(location)\n",
    "        \n",
    "# Convert to function\n",
    "def loc_func(soup, location_list=[]):\n",
    "    for row in soup.find_all('div', {'id':True}):\n",
    "        for header in row.find_all('span', {'class':'location'}):\n",
    "            location = header.text\n",
    "            location_list.append(location)\n",
    "    return location_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no salary info\n",
      "no salary info\n",
      "no salary info\n",
      "no salary info\n",
      "$75,000 - $100,000 a year\n",
      "$120,000 a year\n",
      "no salary info\n",
      "no salary info\n",
      "$81,250 - $100,600 a year\n",
      "no salary info\n",
      "no salary info\n",
      "no salary info\n",
      "no salary info\n",
      "no salary info\n",
      "no salary info\n"
     ]
    }
   ],
   "source": [
    "# Looking at salary if there is one\n",
    "salary_list = []\n",
    "for row in soup.find_all('div', {'id':True}):\n",
    "    for block in row.find_all('td', {'class':'snip'}):\n",
    "        try:\n",
    "            salary =  block.find('span', {'class':'no-wrap'}).text\n",
    "            print salary\n",
    "            salary_list.append(salary)\n",
    "        except:\n",
    "            print 'no salary info'\n",
    "            salary_list.append(np.nan)\n",
    "            \n",
    "def sal_func(soup, salary_list=[]):\n",
    "    for row in soup.find_all('div', {'id':True}):\n",
    "        for block in row.find_all('td', {'class':'snip'}):\n",
    "            try:\n",
    "                salary =  block.find('span', {'class':'no-wrap'}).text\n",
    "                salary_list.append(salary)\n",
    "            except:\n",
    "                salary_list.append(np.nan)\n",
    "                \n",
    "    return salary_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Looking at company\n",
    "company_list = []\n",
    "for row in soup.find_all('div', {'id':True}):\n",
    "    for header in row.find_all('span', {'class':'company'}):\n",
    "        company_list.append(header.text.replace('\\n',''))\n",
    "\n",
    "def comp_func(soup, company_list=[]):\n",
    "    for row in soup.find_all('div', {'id':True}):\n",
    "        for header in row.find_all('span', {'class':'company'}):\n",
    "            company_list.append(header.text.replace('\\n',''))\n",
    "    return company_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sponsored\n",
      "Sponsored\n",
      "Sponsored\n",
      "14 days ago\n",
      "10 days ago\n",
      "13 days ago\n",
      "11 days ago\n",
      "21 days ago\n",
      "5 days ago\n",
      "25 days ago\n",
      "2 days ago\n",
      "10 days ago\n",
      "1 day ago\n",
      "Sponsored\n",
      "Sponsored\n"
     ]
    }
   ],
   "source": [
    "# Looking at days posted/sponsored. Will specify sponsored as np.nan\n",
    "# Time is organized by: hours, days, and more than 30 days. 30+ days will be regarded as 30 days\n",
    "time_list = []\n",
    "for row in soup.find_all('div', {'id':True}):\n",
    "    for header in row.find_all('div', {'class':'result-link-bar'}):\n",
    "        try:\n",
    "            time =  header.find('span', {'class':'date'}).text\n",
    "            print time\n",
    "            if 'days' in time:\n",
    "                days = float(time.split()[0])\n",
    "                time_list.append(round(days,2))\n",
    "            elif 'hours' in time:\n",
    "                hours = float(time.split()[0])/24.\n",
    "                time_list.append(round(hours,2))\n",
    "            else:\n",
    "                time_list.append(30.0)\n",
    "        except:\n",
    "            print 'Sponsored'\n",
    "            time_list.append(np.nan)\n",
    "            \n",
    "def time_func(soup, time_list=[]):\n",
    "    for row in soup.find_all('div', {'id':True}):\n",
    "        for header in row.find_all('div', {'class':'result-link-bar'}):\n",
    "            try:\n",
    "                time =  header.find('span', {'class':'date'}).text\n",
    "                if 'days' in time:\n",
    "                    days = float(time.split()[0])\n",
    "                    time_list.append(days)\n",
    "                elif 'hours' in time:\n",
    "                    hours = float(time.split()[0])/24.\n",
    "                    time_list.append(hours)\n",
    "                else:\n",
    "                    time_list.append(30.0)\n",
    "            except:\n",
    "                time_list.append(np.nan)\n",
    "                \n",
    "    return time_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ratings and number of reviews\n",
    "rating_list = []\n",
    "for row in soup.find_all('div', {'id':True, 'data-jk':True}):\n",
    "    try:\n",
    "        # Get number of ratings\n",
    "        ratings = row.find('span', {'class':'slNoUnderline'}).text\n",
    "        \n",
    "        # Get actual star count out of 5\n",
    "        # width is a string with format 'width:##.#px'. Trying to pull out ##.# as a float\n",
    "        width = row.find('span', {'style':True})['style']\n",
    "        pixels = re.search('\\d{2}.\\d',width)\n",
    "        stars = round(float(pixels.group())/60. * 5, 2)\n",
    "        rating_list.append((stars, ratings))\n",
    "        \n",
    "    except:\n",
    "        rating_list.append((np.nan, np.nan))\n",
    "        \n",
    "rating_list\n",
    "\n",
    "def rat_func(soup, rating_list=[], star_list=[], review_list=[]):\n",
    "    for row in soup.find_all('div', {'id':True, 'data-jk':True}):\n",
    "        try:\n",
    "            # Get number of ratings\n",
    "            ratings = row.find('span', {'class':'slNoUnderline'}).text\n",
    "\n",
    "            # Get actual star count out of 5\n",
    "            # width is a string with format 'width:##.#px'. Trying to pull out ##.# as a float\n",
    "            width = row.find('span', {'style':True})['style']\n",
    "            pixels = re.search('\\d{2}.\\d',width)\n",
    "            stars = round(float(pixels.group())/60. * 5, 2)\n",
    "            rating_list.append((stars, ratings))\n",
    "\n",
    "        except:\n",
    "            rating_list.append((np.nan, np.nan))\n",
    "            \n",
    "    star_list = [i[0] for i in rating_list]\n",
    "    review_list = [i[1] for i in rating_list]\n",
    "            \n",
    "    return star_list, review_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dataframe of information\n",
    "# Lists are:\n",
    "'''job_list, location_list, salary_list, company_list, time_list, rating_list, summary_list\n",
    "rating_list is a tuple with two pieces of info: rating out of 5 and number of ratings'''\n",
    "\n",
    "# Split rating_list into 2 parts: star_list and review_list\n",
    "star_list = [i[0] for i in rating_list]\n",
    "review_list = [i[1] for i in rating_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get summary of job posting\n",
    "summary_list = []\n",
    "for row in soup.find_all('div', {'id':True}):\n",
    "    for header in row.find_all('span', {'class':'summary'}):\n",
    "        summary_list.append(header.text.replace('\\n',''))\n",
    "\n",
    "def sum_func(soup, summary_list=[]):\n",
    "    for row in soup.find_all('div', {'id':True}):\n",
    "        for header in row.find_all('span', {'class':'summary'}):\n",
    "            summary_list.append(header.text.replace('\\n',''))\n",
    "    return summary_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Location</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Company</th>\n",
       "      <th>Post_date</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Arlington, VA 22209</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Deloitte</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.25</td>\n",
       "      <td>4,610 reviews</td>\n",
       "      <td>Gathering data from both information systems a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Durham, NC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xometry</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Xometry is seeking a data scientist to build t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Harrisburg, PA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pennsylvania Higher Education Assistance A...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.35</td>\n",
       "      <td>201 reviews</td>\n",
       "      <td>Scrubbing data if required. Assist in text ana...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist, North America Supply Chain Adv...</td>\n",
       "      <td>Portland, OR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NIKE INC</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.35</td>\n",
       "      <td>3,377 reviews</td>\n",
       "      <td>North America Supply Chain Advanced Analytics ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>United States</td>\n",
       "      <td>$75,000 - $100,000 a year</td>\n",
       "      <td>RDM</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.35</td>\n",
       "      <td>14 reviews</td>\n",
       "      <td>A deep understanding of algorithms, mathematic...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title             Location  \\\n",
       "0                                     Data Scientist  Arlington, VA 22209   \n",
       "1                                     Data Scientist           Durham, NC   \n",
       "2                                     Data Scientist       Harrisburg, PA   \n",
       "3  Data Scientist, North America Supply Chain Adv...         Portland, OR   \n",
       "4                                     Data Scientist        United States   \n",
       "\n",
       "                      Salary  \\\n",
       "0                        NaN   \n",
       "1                        NaN   \n",
       "2                        NaN   \n",
       "3                        NaN   \n",
       "4  $75,000 - $100,000 a year   \n",
       "\n",
       "                                             Company  Post_date  Rating  \\\n",
       "0                                           Deloitte        NaN    4.25   \n",
       "1                                            xometry        NaN     NaN   \n",
       "2      Pennsylvania Higher Education Assistance A...        NaN    3.35   \n",
       "3                                           NIKE INC       14.0    4.35   \n",
       "4                                                RDM       10.0    4.35   \n",
       "\n",
       "         Reviews                                            Summary  \n",
       "0  4,610 reviews  Gathering data from both information systems a...  \n",
       "1            NaN  Xometry is seeking a data scientist to build t...  \n",
       "2    201 reviews  Scrubbing data if required. Assist in text ana...  \n",
       "3  3,377 reviews  North America Supply Chain Advanced Analytics ...  \n",
       "4     14 reviews  A deep understanding of algorithms, mathematic...  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(zip(job_func(soup), \n",
    "                      loc_func(soup), \n",
    "                      sal_func(soup), \n",
    "                      comp_func(soup), \n",
    "                      time_func(soup), \n",
    "                      rat_func(soup)[0], \n",
    "                      rat_func(soup)[1],\n",
    "                      sum_func(soup)), \n",
    "                  columns=['Title', \n",
    "                           'Location', \n",
    "                           'Salary',\n",
    "                           'Company', \n",
    "                           'Post_date', \n",
    "                           'Rating', \n",
    "                           'Reviews',\n",
    "                           'Summary'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Got some information, but df is not clean at all. Let's work easiest to hardest. \n",
    "# Reviews is first\n",
    "\n",
    "def cleaner(row):\n",
    "    try:\n",
    "        # Replace commas with nothing\n",
    "        new_row = row.replace(',', '')\n",
    "\n",
    "        # Get of rid of word 'reviews'\n",
    "        new_row = float(new_row.split()[0])\n",
    "    except:\n",
    "        new_row = np.nan    \n",
    "    \n",
    "    return new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a cleaner for salary. Taking average of range of salary for computation purposes\n",
    "def sal_cleaner(row):\n",
    "    try:\n",
    "        # Replace commas with nothing\n",
    "        new_row = row.replace(',', '')\n",
    "\n",
    "        # Get list of salary split by -\n",
    "        split_list = new_row.split('a year')[0]\n",
    "        dol_replace = split_list.replace('$', '')\n",
    "        dol_split = dol_replace.split('-')\n",
    "        \n",
    "        if len(dol_split) > 1:\n",
    "            new_row = 0.5 * (float(dol_split[0]) + float(dol_split[1]))\n",
    "        else:\n",
    "            new_row = float(dol_split[0])\n",
    "    except:\n",
    "        new_row = np.nan    \n",
    "    \n",
    "    return new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We are going to eventually run this above code multiple times. So better to put in a function\n",
    "def indeed_maker(soup):\n",
    "    df = pd.DataFrame(zip(job_func(soup), \n",
    "                      loc_func(soup), \n",
    "                      sal_func(soup), \n",
    "                      comp_func(soup), \n",
    "                      time_func(soup), \n",
    "                      rat_func(soup)[0], \n",
    "                      rat_func(soup)[1],\n",
    "                      sum_func(soup)), \n",
    "                  columns=['Title', \n",
    "                           'Location', \n",
    "                           'Salary',\n",
    "                           'Company', \n",
    "                           'Post_date', \n",
    "                           'Rating', \n",
    "                           'Reviews',\n",
    "                           'Summary'])\n",
    "    \n",
    "    df.Reviews = df.Reviews.map(cleaner)\n",
    "    df.Salary = df.Salary.map(sal_cleaner)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-118-e8e633fdc5df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[0mnotnull_sum\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m150\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'https://www.indeed.com/jobs?q=data+scientist+$20,000&start='\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mhtml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lxml'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Chris\\Anaconda3\\envs\\python2\\lib\\socket.pyc\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    353\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m                     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrbufsize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mEINTR\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Chris\\Anaconda3\\envs\\python2\\lib\\ssl.pyc\u001b[0m in \u001b[0;36mrecv\u001b[1;34m(self, buflen, flags)\u001b[0m\n\u001b[0;32m    764\u001b[0m                     \u001b[1;34m\"non-zero flags not allowed in calls to recv() on %s\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    765\u001b[0m                     self.__class__)\n\u001b[1;32m--> 766\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    767\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    768\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Chris\\Anaconda3\\envs\\python2\\lib\\ssl.pyc\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m    651\u001b[0m                 \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    652\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 653\u001b[1;33m                 \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    654\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    655\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Running script so that we get 120 salaries\n",
    "# Testing with 20 first\n",
    "notnull_sum = 0\n",
    "start = '00'\n",
    "while notnull_sum < 150:\n",
    "    url = 'https://www.indeed.com/jobs?q=data+scientist+$20,000&start=' + start\n",
    "    html = urllib.urlopen(url).read()\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    \n",
    "    ind_df = indeed_maker(soup)\n",
    "    ind_df.drop_duplicates(inplace=True)\n",
    "    notnull_sum = ind_df.Salary.notnull().sum()\n",
    "    start = int(start) + 10\n",
    "    start = str(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Saving dataframe as a csv so that I don't have to run code again.\n",
    "ind_df.to_csv('Indeed_0612', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Indeed_0612'\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
