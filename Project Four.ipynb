{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://www.indeed.com/jobs?q=data+scientist+$20,000&l=New+York&start=30\n",
    "# $20000 means 20k or up\n",
    "# l = location\n",
    "# start increases incrementally by 10. Starting at 00."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trialing one page of Indeed to test if my functions work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test one page of url\n",
    "url = 'https://www.indeed.com/jobs?q=data+scientist&start=00'\n",
    "html = urllib.urlopen(url).read()\n",
    "soup = BeautifulSoup(html, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "break\n"
     ]
    }
   ],
   "source": [
    "rows = soup.find_all('span', {'class':'np'})\n",
    "if len(rows) > 1:\n",
    "    print 'pass'\n",
    "elif 'Next' in rows[0].text:\n",
    "    print 'break'\n",
    "else:\n",
    "    print 'what'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Next\\xa0\\xbb'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function making\n",
    "Defining all my functions for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Scientist\n",
      "Data Scientist\n",
      "Junior Data Scientist\n",
      "Jr Data Scientist\n",
      "Data Scientist\n",
      "Data Scientist/Machine Learning Engineer\n",
      "Data Scientist\n",
      "Data Scientist\n",
      "Data Science Analyst\n",
      "Data Scientist\n",
      "Data Scientist\n",
      "Entry Level Data Scientist\n",
      "Data Scientist (Product)\n",
      "Data Scientist - Interactive Gaming\n",
      "Data Scientist - Machine Learning\n"
     ]
    }
   ],
   "source": [
    "# Trying to get all jobs titles first\n",
    "job_list = []\n",
    "for row in soup.find_all('div', {'id':True, 'data-jk':True}):\n",
    "    jobtitle = row.a.text\n",
    "    print jobtitle\n",
    "    job_list.append(jobtitle)\n",
    "    \n",
    "# Convert to function\n",
    "def job_func(soup, job_list=[]):\n",
    "    for row in soup.find_all('div', {'id':True, 'data-jk':True}):\n",
    "        jobtitle = row.a.text\n",
    "        job_list.append(jobtitle)\n",
    "    return job_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Durham, NC\n",
      "Arlington, VA 22209\n",
      "Tempe, AZ 85282\n",
      "Chantilly, VA 20151\n",
      "Kennesaw, GA\n",
      "San Diego, CA 92129\n",
      "Seattle, WA 98103 (Green Lake - Wallingford area)\n",
      "Newark, DE\n",
      "Silver Spring, MD 20910\n",
      "Chicago, IL\n",
      "Kansas City, MO\n",
      "United States\n",
      "New York, NY 10011 (Chelsea area)\n",
      "San Francisco, CA\n",
      "Palo Alto, CA\n"
     ]
    }
   ],
   "source": [
    "# Looking at location\n",
    "location_list = []\n",
    "for row in soup.find_all('div', {'id':True}):\n",
    "    for header in row.find_all('span', {'class':'location'}):\n",
    "        location = header.text\n",
    "        print location\n",
    "        location_list.append(location)\n",
    "        \n",
    "# Convert to function\n",
    "def loc_func(soup, location_list=[]):\n",
    "    for row in soup.find_all('div', {'id':True}):\n",
    "        for header in row.find_all('span', {'class':'location'}):\n",
    "            location = header.text\n",
    "            location_list.append(location)\n",
    "    return location_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no salary info\n",
      "no salary info\n",
      "no salary info\n",
      "no salary info\n",
      "no salary info\n",
      "no salary info\n",
      "no salary info\n",
      "$125,000 a year\n",
      "no salary info\n",
      "$110,000 - $125,000 a year\n",
      "no salary info\n",
      "no salary info\n",
      "no salary info\n",
      "no salary info\n",
      "no salary info\n"
     ]
    }
   ],
   "source": [
    "# Looking at salary if there is one\n",
    "salary_list = []\n",
    "for row in soup.find_all('div', {'id':True}):\n",
    "    for block in row.find_all('td', {'class':'snip'}):\n",
    "        try:\n",
    "            salary =  block.find('span', {'class':'no-wrap'}).text\n",
    "            print salary\n",
    "            salary_list.append(salary)\n",
    "        except:\n",
    "            print 'no salary info'\n",
    "            salary_list.append(np.nan)\n",
    "            \n",
    "def sal_func(soup, salary_list=[]):\n",
    "    for row in soup.find_all('div', {'id':True}):\n",
    "        for block in row.find_all('td', {'class':'snip'}):\n",
    "            try:\n",
    "                salary =  block.find('span', {'class':'no-wrap'}).text\n",
    "                salary_list.append(salary)\n",
    "            except:\n",
    "                salary_list.append(np.nan)\n",
    "                \n",
    "    return salary_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Looking at company\n",
    "company_list = []\n",
    "for row in soup.find_all('div', {'id':True}):\n",
    "    for header in row.find_all('span', {'class':'company'}):\n",
    "        company_list.append(header.text.replace('\\n',''))\n",
    "\n",
    "def comp_func(soup, company_list=[]):\n",
    "    for row in soup.find_all('div', {'id':True}):\n",
    "        for header in row.find_all('span', {'class':'company'}):\n",
    "            company_list.append(header.text.replace('\\n',''))\n",
    "    return company_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sponsored\n",
      "Sponsored\n",
      "Sponsored\n",
      "1 day ago\n",
      "3 days ago\n",
      "1 day ago\n",
      "2 days ago\n",
      "2 days ago\n",
      "9 hours ago\n",
      "1 day ago\n",
      "5 days ago\n",
      "1 day ago\n",
      "1 day ago\n",
      "Sponsored\n",
      "Sponsored\n"
     ]
    }
   ],
   "source": [
    "# Looking at days posted/sponsored. Will specify sponsored as np.nan\n",
    "# Time is organized by: hours, days, and more than 30 days. 30+ days will be regarded as 30 days\n",
    "time_list = []\n",
    "for row in soup.find_all('div', {'id':True}):\n",
    "    for header in row.find_all('div', {'class':'result-link-bar'}):\n",
    "        try:\n",
    "            time =  header.find('span', {'class':'date'}).text\n",
    "            print time\n",
    "            if 'days' in time:\n",
    "                days = float(time.split()[0])\n",
    "                time_list.append(round(days,2))\n",
    "            elif 'hours' in time:\n",
    "                hours = float(time.split()[0])/24.\n",
    "                time_list.append(round(hours,2))\n",
    "            else:\n",
    "                time_list.append(30.0)\n",
    "        except:\n",
    "            print 'Sponsored'\n",
    "            time_list.append(np.nan)\n",
    "            \n",
    "def time_func(soup, time_list=[]):\n",
    "    for row in soup.find_all('div', {'id':True}):\n",
    "        for header in row.find_all('div', {'class':'result-link-bar'}):\n",
    "            try:\n",
    "                time =  header.find('span', {'class':'date'}).text\n",
    "                if 'days' in time:\n",
    "                    days = float(time.split()[0])\n",
    "                    time_list.append(days)\n",
    "                elif 'hours' in time:\n",
    "                    hours = float(time.split()[0])/24.\n",
    "                    time_list.append(hours)\n",
    "                else:\n",
    "                    time_list.append(30.0)\n",
    "            except:\n",
    "                time_list.append(np.nan)\n",
    "                \n",
    "    return time_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make dataframe of information\n",
    "# # Lists are:\n",
    "# '''job_list, location_list, salary_list, company_list, time_list, rating_list, summary_list\n",
    "# rating_list is a tuple with two pieces of info: rating out of 5 and number of ratings'''\n",
    "\n",
    "# # Split rating_list into 2 parts: star_list and review_list\n",
    "# star_list = [i[0] for i in rating_list]\n",
    "# review_list = [i[1] for i in rating_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get summary of job posting\n",
    "summary_list = []\n",
    "for row in soup.find_all('div', {'id':True}):\n",
    "    for header in row.find_all('span', {'class':'summary'}):\n",
    "        summary_list.append(header.text.replace('\\n',''))\n",
    "\n",
    "def sum_func(soup, summary_list=[]):\n",
    "    for row in soup.find_all('div', {'id':True}):\n",
    "        for header in row.find_all('span', {'class':'summary'}):\n",
    "            summary_list.append(header.text.replace('\\n',''))\n",
    "    return summary_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ratings and number of reviews\n",
    "rating_list = []\n",
    "for row in soup.find_all('div', {'id':True, 'data-jk':True}):\n",
    "    try:\n",
    "        # Get number of ratings\n",
    "        ratings = row.find('span', {'class':'slNoUnderline'}).text\n",
    "        \n",
    "        # Get actual star count out of 5\n",
    "        # width is a string with format 'width:##.#px'. Trying to pull out ##.# as a float\n",
    "        width = row.find('span', {'style':True})['style']\n",
    "        pixels = re.search('\\d{2}.\\d',width)\n",
    "        stars = round(float(pixels.group())/60. * 5, 2)\n",
    "        rating_list.append((stars, ratings))\n",
    "        \n",
    "    except:\n",
    "        rating_list.append((np.nan, np.nan))\n",
    "\n",
    "def rat_func(soup, rating_list=[], star_list=[], review_list=[]):\n",
    "    for row in soup.find_all('div', {'id':True, 'data-jk':True}):\n",
    "        try:\n",
    "            # Get number of ratings\n",
    "            ratings = row.find('span', {'class':'slNoUnderline'}).text\n",
    "\n",
    "            # Get actual star count out of 5\n",
    "            # width is a string with format 'width:##.#px'. Trying to pull out ##.# as a float\n",
    "            width = row.find('span', {'style':True})['style']\n",
    "            pixels = re.search('\\d{2}.\\d',width)\n",
    "            stars = round(float(pixels.group())/60. * 5, 2)\n",
    "            rating_list.append((stars, ratings))\n",
    "\n",
    "        except:\n",
    "            rating_list.append((np.nan, np.nan))\n",
    "            \n",
    "    star_list = [i[0] for i in rating_list]\n",
    "    review_list = [i[1] for i in rating_list]\n",
    "            \n",
    "    return star_list, review_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(zip(job_func(soup), \n",
    "#                       loc_func(soup), \n",
    "#                       sal_func(soup), \n",
    "#                       comp_func(soup), \n",
    "#                       time_func(soup), \n",
    "#                       rat_func(soup)[0], \n",
    "#                       rat_func(soup)[1],\n",
    "#                       sum_func(soup)), \n",
    "#                   columns=['Title', \n",
    "#                            'Location', \n",
    "#                            'Salary',\n",
    "#                            'Company', \n",
    "#                            'Post_date', \n",
    "#                            'Rating', \n",
    "#                            'Reviews',\n",
    "#                            'Summary'])\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Got some information, but df is not clean at all. Let's work easiest to hardest. \n",
    "# Reviews is first\n",
    "\n",
    "def cleaner(row):\n",
    "    try:\n",
    "        # Replace commas with nothing\n",
    "        new_row = row.replace(',', '')\n",
    "\n",
    "        # Get of rid of word 'reviews'\n",
    "        new_row = float(new_row.split()[0])\n",
    "    except:\n",
    "        new_row = np.nan    \n",
    "    \n",
    "    return new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a cleaner for salary. Taking average of range of salary for computation purposes\n",
    "def sal_cleaner(row):\n",
    "    try:\n",
    "        # Replace commas with nothing\n",
    "        new_row = row.replace(',', '')\n",
    "\n",
    "        # Get list of salary split by -\n",
    "        split_list = new_row.split('a year')[0]\n",
    "        dol_replace = split_list.replace('$', '')\n",
    "        dol_split = dol_replace.split('-')\n",
    "        \n",
    "        if len(dol_split) > 1:\n",
    "            new_row = 0.5 * (float(dol_split[0]) + float(dol_split[1]))\n",
    "        else:\n",
    "            new_row = float(dol_split[0])\n",
    "    except:\n",
    "        new_row = np.nan    \n",
    "    \n",
    "    return new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We are going to eventually run this above code multiple times. So better to put in a function\n",
    "def indeed_maker(soup):\n",
    "    df = pd.DataFrame(zip(job_func(soup), \n",
    "                      loc_func(soup), \n",
    "                      sal_func(soup), \n",
    "                      comp_func(soup), \n",
    "                      time_func(soup), \n",
    "                      rat_func(soup)[0], \n",
    "                      rat_func(soup)[1],\n",
    "                      sum_func(soup)), \n",
    "                  columns=['Title', \n",
    "                           'Location', \n",
    "                           'Salary',\n",
    "                           'Company', \n",
    "                           'Post_date', \n",
    "                           'Rating', \n",
    "                           'Reviews',\n",
    "                           'Summary'])\n",
    "    \n",
    "    df.Reviews = df.Reviews.map(cleaner)\n",
    "    df.Salary = df.Salary.map(sal_cleaner)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to run. Tried to run a while loop, but Indeed cuts off at page 100.\n",
    "Even if there are more than 100 page worths, Indeed cuts off.\n",
    "As a result, will have to search through several cities instead of searching without locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_of_cities = ['Atlanta', 'Los+Angeles', 'Seattle', 'Portland', 'San+Francisco', \n",
    "                  'Washington%2C+DC', 'Boston', 'Austin', 'Dallas', 'Houston',\n",
    "                  'Orlando', 'Philadelphia', 'Pittsburgh', 'Cincinnati', 'Miami',\n",
    "                  'New+York+City', 'New+Jersey', 'Kansas+City', 'San Diego',\n",
    "                 'San+Jose', 'Salt+Lake+City', 'Raleigh', 'Minneapolis', 'Oklahoma+City',\n",
    "                 'St.+Louis', 'Detroit', 'Colombus', 'Cleveland', 'Indianapolis', 'Baltimore']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking for two titles\n",
    "for role in ['data+scientist', 'data+analyst']:\n",
    "    # Looking at a list of cities\n",
    "    for city in list_of_cities:\n",
    "        # Looking through all pages Indeed will allow you to look through\n",
    "        for page in np.linspace(0, 950, 20):\n",
    "            # Page indicator\n",
    "            start = str(int(page))\n",
    "\n",
    "            # Web initializer\n",
    "            url = 'https://www.indeed.com/jobs?q=' + role + '&l=' + city + '&limit=50&start=' + start\n",
    "            html = urllib.urlopen(url).read()\n",
    "            soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "            # Function that scrapes all relevant data and creates a dataframe        \n",
    "            ind_df = indeed_maker(soup)\n",
    "            \n",
    "            # Break for loop if no next link is available\n",
    "#             rows = soup.find_all('span', {'class':'np'})\n",
    "#             if len(rows) > 1:\n",
    "#                 pass\n",
    "#             elif 'Previous' in rows[0].text:\n",
    "#                 break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Salary</th>\n",
       "      <th>Post_date</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1871.000000</td>\n",
       "      <td>20417.000000</td>\n",
       "      <td>29441.000000</td>\n",
       "      <td>29441.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>86023.212453</td>\n",
       "      <td>13.945187</td>\n",
       "      <td>3.904609</td>\n",
       "      <td>1591.476648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>38038.561864</td>\n",
       "      <td>9.074210</td>\n",
       "      <td>0.497423</td>\n",
       "      <td>5608.328746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>10000.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>59889.750000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.550000</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>77500.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>156.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>102500.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>768.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>275000.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>98543.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Salary     Post_date        Rating       Reviews\n",
       "count    1871.000000  20417.000000  29441.000000  29441.000000\n",
       "mean    86023.212453     13.945187      3.904609   1591.476648\n",
       "std     38038.561864      9.074210      0.497423   5608.328746\n",
       "min     10000.000000      0.083333      1.250000      2.000000\n",
       "25%     59889.750000      5.000000      3.550000     27.000000\n",
       "50%     77500.000000     12.000000      3.700000    156.000000\n",
       "75%    102500.000000     22.000000      4.350000    768.000000\n",
       "max    275000.000000     30.000000      5.000000  98543.000000"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting rid of row duplicates (sometimes reviews and rating will be different)\n",
    "ind_df.drop_duplicates(subset=['Title', 'Location', 'Salary', 'Company', 'Summary'], \n",
    "                       inplace=True)\n",
    "ind_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39020, 8)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Salary</th>\n",
       "      <th>Post_date</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1871.000000</td>\n",
       "      <td>20417.000000</td>\n",
       "      <td>29441.000000</td>\n",
       "      <td>29441.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>86023.212453</td>\n",
       "      <td>13.945187</td>\n",
       "      <td>3.904609</td>\n",
       "      <td>1591.476648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>38038.561864</td>\n",
       "      <td>9.074210</td>\n",
       "      <td>0.497423</td>\n",
       "      <td>5608.328746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>10000.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>59889.750000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.550000</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>77500.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>156.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>102500.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>768.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>275000.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>98543.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Salary     Post_date        Rating       Reviews\n",
       "count    1871.000000  20417.000000  29441.000000  29441.000000\n",
       "mean    86023.212453     13.945187      3.904609   1591.476648\n",
       "std     38038.561864      9.074210      0.497423   5608.328746\n",
       "min     10000.000000      0.083333      1.250000      2.000000\n",
       "25%     59889.750000      5.000000      3.550000     27.000000\n",
       "50%     77500.000000     12.000000      3.700000    156.000000\n",
       "75%    102500.000000     22.000000      4.350000    768.000000\n",
       "max    275000.000000     30.000000      5.000000  98543.000000"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Saving dataframe as a csv so that I don't have to run code again.\n",
    "ind_df.to_csv('Indeed_0618', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "File Indeed_0616 does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-1a966cfe29f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Indeed_0616'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\Chris\\Anaconda3\\envs\\python2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    644\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    645\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 646\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    647\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Chris\\Anaconda3\\envs\\python2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 389\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Chris\\Anaconda3\\envs\\python2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    728\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 730\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    731\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    732\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Chris\\Anaconda3\\envs\\python2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m    921\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    922\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 923\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    924\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    925\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Chris\\Anaconda3\\envs\\python2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1388\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1389\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1390\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1392\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas\\parser.c:4184)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas\\parser.c:8449)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: File Indeed_0616 does not exist"
     ]
    }
   ],
   "source": [
    "path = 'Indeed_0616'\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dropped.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
